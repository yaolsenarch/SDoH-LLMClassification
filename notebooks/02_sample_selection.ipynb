{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041f79fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path: ['/usr/lib64/python311.zip', '/usr/lib64/python3.11', '/usr/lib64/python3.11/lib-dynload', '', '/proj/sas/ripython/venv_311/lib64/python3.11/site-packages', '/proj/sas/ripython/venv_311/lib/python3.11/site-packages']\n",
      "\n",
      "llm_utils location: /proj/sas/ripython/LLMClassfication/GitHub/SDoH-LLMClassification/llm_utils.py\n",
      "\n",
      "Contents of llm_utils: ['AzureOpenAI', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'ast', 'calculate_disagreement_score', 'calculate_multilabel_metrics', 'cohen_kappa_score', 'convert_gold_labels_to_list', 'create_2shot_hard_prompt', 'generate_explanation_for_easy_example', 'generate_explanation_for_hard_example', 'get_prompt_template', 'get_strengthened_prompt_template', 'json', 'np', 'os', 'parse_llm_labels', 'precision_recall_fscore_support', 're', 'run_llm_annotation', 'run_zero_shot_and_score', 'select_easy_examples_automatically', 'select_hard_examples_automatically', 'time']\n",
      "\n",
      "After reload - Contents of llm_utils: ['AzureOpenAI', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'ast', 'calculate_disagreement_score', 'calculate_multilabel_metrics', 'cohen_kappa_score', 'convert_gold_labels_to_list', 'create_2shot_hard_prompt', 'generate_explanation_for_easy_example', 'generate_explanation_for_hard_example', 'get_prompt_template', 'get_strengthened_prompt_template', 'json', 'np', 'os', 'parse_llm_labels', 'precision_recall_fscore_support', 're', 'run_llm_annotation', 'run_zero_shot_and_score', 'select_easy_examples_automatically', 'select_hard_examples_automatically', 'time']\n"
     ]
    }
   ],
   "source": [
    "# Try importing with debug info\n",
    "import sys\n",
    "print(\"Python path:\", sys.path)\n",
    "\n",
    "import llm_utils\n",
    "print(\"\\nllm_utils location:\", llm_utils.__file__)\n",
    "print(\"\\nContents of llm_utils:\", dir(llm_utils))\n",
    "\n",
    "# Try reloading the module to get latest changes\n",
    "import importlib\n",
    "importlib.reload(llm_utils)\n",
    "print(\"\\nAfter reload - Contents of llm_utils:\", dir(llm_utils))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90afe373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "from sdoh_data import load_sdoh_dataset   # âœ… helper module\n",
    "from llm_utils import (parse_llm_labels, run_llm_annotation, calculate_disagreement_score, run_zero_shot_and_score,select_hard_examples_automatically,\n",
    "                        generate_explanation_for_hard_example, create_2shot_hard_prompt, select_easy_examples_automatically, generate_explanation_for_easy_example,\n",
    "                        calculate_multilabel_metrics)\n",
    "from sklearn.metrics import precision_recall_fscore_support, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6a2ea",
   "metadata": {},
   "source": [
    "### Best Practice for Multi-Label Hard Example Selection:Hamming Loss Approach\n",
    "1) Handles class imbalance naturally - We can weight by category frequency\n",
    "2) Provides rich teaching examples - An example that's wrong on multiple categories teaches the model more\n",
    "3) Standard in multi-label literature - Hamming distance/loss is the go-to metric for multi-label disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5e03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1, Errors: {'alcohol': 'FN (missed)'}\n",
      "Score: 1, Errors: {'employment': 'FP (false alarm)'}\n",
      "Score: 4, Errors: {'employment': 'FP (false alarm)', 'housing': 'FP (false alarm)', 'smoking': 'FN (missed)', 'alcohol': 'FN (missed)'}\n"
     ]
    }
   ],
   "source": [
    "categories = [\"employment\", \"housing\", \"smoking\", \"alcohol\"]\n",
    "\n",
    "# Example 1: Model misses \"alcohol\"\n",
    "gold = [\"smoking\", \"alcohol\"]\n",
    "pred = [\"smoking\"]\n",
    "score, errors = calculate_disagreement_score(gold, pred, categories)\n",
    "print(f\"Score: {score}, Errors: {errors}\")\n",
    "# Output: Score: 1, Errors: {'alcohol': 'FN (missed)'}\n",
    "\n",
    "# Example 2: Model adds \"employment\" wrongly\n",
    "gold = [\"smoking\"]\n",
    "pred = [\"smoking\", \"employment\"]\n",
    "score, errors = calculate_disagreement_score(gold, pred, categories)\n",
    "print(f\"Score: {score}, Errors: {errors}\")\n",
    "# Output: Score: 1, Errors: {'employment': 'FP (false alarm)'}\n",
    "\n",
    "# Example 3: Multiple errors\n",
    "gold = [\"smoking\", \"alcohol\"]\n",
    "pred = [\"employment\", \"housing\"]\n",
    "score, errors = calculate_disagreement_score(gold, pred, categories)\n",
    "print(f\"Score: {score}, Errors: {errors}\")\n",
    "# Output: Score: 4, Errors: {...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a451a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 0-shot on 10 samples...\n",
      "\n",
      "Completed! Disagreement score stats:\n",
      "count    10.0\n",
      "mean      0.0\n",
      "std       0.0\n",
      "min       0.0\n",
      "25%       0.0\n",
      "50%       0.0\n",
      "75%       0.0\n",
      "max       0.0\n",
      "Name: disagreement_score, dtype: float64\n",
      "                                               premise     gold_labels  \\\n",
      "507  The patient is employed in the finance department              []   \n",
      "70   He also quit smoking cigarettes in 1984, after...     ['smoking']   \n",
      "131                    He has occasional glass of wine     ['alcohol']   \n",
      "400  She is presently unemployed, but plans to go o...  ['employment']   \n",
      "541  The patient smokes about a pack a day for more...     ['smoking']   \n",
      "\n",
      "    predicted_labels  disagreement_score  \n",
      "507               []                   0  \n",
      "70         [smoking]                   0  \n",
      "131        [alcohol]                   0  \n",
      "400     [employment]                   0  \n",
      "541        [smoking]                   0  \n"
     ]
    }
   ],
   "source": [
    "# Test run with 10 samples\n",
    "df = load_sdoh_dataset()\n",
    "categories = [c for c in df.columns if c != \"premise\"]\n",
    "categories\n",
    "df_gold = pd.read_csv(\"annotations_with_gold.csv\")\n",
    "\n",
    "test_results = run_zero_shot_and_score(\n",
    "    df=df_gold,  # Your dataframe with gold labels\n",
    "    categories=categories,  # Your 10 categories\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    sample_size=10  # Small test\n",
    ")\n",
    "\n",
    "# Check the results\n",
    "print(test_results[['premise', 'gold_labels', 'predicted_labels', 'disagreement_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1d4101",
   "metadata": {},
   "source": [
    "***Total 570 rows, from which 50 records are sampled for creating a variety of shots that will be used by LLM to label the test set.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84fd9221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 0-shot on 50 samples...\n",
      "\n",
      "Completed! Disagreement score stats:\n",
      "count    50.000000\n",
      "mean      0.040000\n",
      "std       0.197949\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       0.000000\n",
      "75%       0.000000\n",
      "max       1.000000\n",
      "Name: disagreement_score, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>gold_labels</th>\n",
       "      <th>predicted_labels</th>\n",
       "      <th>disagreement_score</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>He denies any smoking or drug usage.</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>{'smoking': 'FN (missed)'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>He currently lives with his mother in house for several both<br> in New York and here in Colorado</td>\n",
       "      <td>['housing']</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>{'housing': 'FN (missed)'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>He has occasional glass of wine</td>\n",
       "      <td>['alcohol']</td>\n",
       "      <td>[alcohol]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>She is presently unemployed, but plans to go on a job interv<br>iew today with Alaska USA Federal Credit Union</td>\n",
       "      <td>['employment']</td>\n",
       "      <td>[employment]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>The patient smokes about a pack a day for more than 25 years</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>She has a 27 pack year smoking history</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>He smoked a pack and a half of cigarettes per day for the pa<br>st 35 years</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Denied tobacco/ETOH/illicit drug use.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>He denies alcohol or drug abuse.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>No history of ETOH, Tobacco or illicit drug use</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>Significant history of alcohol abuse, according to the emerg<br>ency room physician, who sees her on a regular basis.</td>\n",
       "      <td>['alcohol']</td>\n",
       "      <td>[alcohol]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Nonsmoker, nondrinker</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>The patient lives with mother, father, and a brother</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Part-time farmer</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>The patient lives with his partner</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>He denies any IV drug use and has an occasional alcohol.</td>\n",
       "      <td>['alcohol']</td>\n",
       "      <td>[alcohol]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>The patient is employed in the finance department</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>He also quit smoking cigarettes in 1984, after 16 years of s<br>moking</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>He is unemployed</td>\n",
       "      <td>['employment']</td>\n",
       "      <td>[employment]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>No smoking or alcohol.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>He continues to smoke one pack of cigarettes daily, as he ha<br>s for the past 28 years</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>He does have smoking history as about a thirteen and a half <br>pack year history of smoking, currently smoking about a quar<br>ter of a pack per day</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>She recently retired from State of Pennsylvania as a psychia<br>tric aide after 32 years of service.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>She is married, lives with her husband, has 2 children that <br>passed away and 4 surviving children</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>Smokes 1-2 packs/cigarettes per day and drinks alcohol socia<br>lly.</td>\n",
       "      <td>['smoking', 'alcohol']</td>\n",
       "      <td>[smoking, alcohol]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Occupation: truck driver</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>The patient rates her general health as excellent and denies<br> any smoking and reports very occasional alcohol consumption</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>Unemployed and formerly worked at an herbicide plant.</td>\n",
       "      <td>['employment']</td>\n",
       "      <td>[employment]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Never smoked</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>He came to Alaska in 1977; his father left his last term of <br>service in the army in Germany at that time, and they came t<br>o Alaska to help a grandparent build a cabin; they ended up <br>staying</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>She consumes alcohol moderately.</td>\n",
       "      <td>['alcohol']</td>\n",
       "      <td>[alcohol]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>She is employed as a manager at the New York department of t<br>axation</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>The patient lives at home with mother, father, and 2 other s<br>iblings</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>No smoking or significant alcohol intake</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>The patient lives with the mother</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Married, unemployed</td>\n",
       "      <td>['employment']</td>\n",
       "      <td>[employment]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Active smoker</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Does not smoke, drink, or utilize illicit substances.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>He smokes one and a half pack a day for 15 years, but he has<br> recently stopped smoking for the past two weeks.</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6-pack beer plus 2 drinks per day for many years: now claims<br> he has been dry for 2 years</td>\n",
       "      <td>['alcohol']</td>\n",
       "      <td>[alcohol]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>He does not drink alcohol</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>She endorses marijuana use and a history of cocaine use five<br> years ago</td>\n",
       "      <td>['marijuana', 'cocaine', 'drug_use']</td>\n",
       "      <td>[marijuana, cocaine, drug_use]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>She has never used illicit drugs</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>He smokes one pack per day</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>He is retired from the social security administration x 20 y<br>ears</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>He does not use alcohol, tobacco, or illicit drugs.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>The patient used to smoke, but quit approximately 30 years a<br>go</td>\n",
       "      <td>['smoking']</td>\n",
       "      <td>[smoking]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Uses no tobacco, alcohol, or illicit drugs</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>He lives alone</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Patient denies illegal drug use</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# Step 1: run 0 shot first to create baseline\n",
    "# =================================================================================\n",
    "from IPython.display import display, HTML\n",
    "test_results = run_zero_shot_and_score(\n",
    "    df=df_gold,  # Your dataframe with gold labels\n",
    "    categories=categories,  # Your 10 categories\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    sample_size=50  \n",
    ")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "test_results.sort_values(\"disagreement_score\", ascending=False, inplace=True)\n",
    "# test_results[['premise', 'gold_labels', 'predicted_labels', 'disagreement_score']]\n",
    "def wrap_text(s, width=60):\n",
    "    return '<br>'.join([s[i:i+width] for i in range(0, len(s), width)])\n",
    "\n",
    "test_results_wrapped = test_results.copy()\n",
    "test_results_wrapped['premise'] = test_results_wrapped['premise'].apply(lambda x: wrap_text(str(x), width=60))\n",
    "display(HTML(test_results_wrapped[['premise', 'gold_labels', 'predicted_labels', 'disagreement_score', 'errors']].to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138137b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically selected hard examples:\n",
      "\n",
      "Hard Example 1:\n",
      "  Premise: He denies any smoking or drug usage....\n",
      "  Gold: ['smoking']\n",
      "  Predicted: []\n",
      "  Score: 1\n",
      "\n",
      "Hard Example 2:\n",
      "  Premise: He currently lives with his mother in house for several both<br> in New York and...\n",
      "  Gold: ['housing']\n",
      "  Predicted: []\n",
      "  Score: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# Step 2a: select hard examples  \n",
    "# =================================================================================\n",
    "hard_examples = select_hard_examples_automatically(test_results_wrapped, top_n=2)\n",
    "\n",
    "print(\"Automatically selected hard examples:\\n\")\n",
    "for i, ex in enumerate(hard_examples, 1): # 1 specifies the starting index. by default, enumerate() starts counting from 0\n",
    "    print(f\"Hard Example {i}:\") # or if by default starting from 0, i here would be i+1 \n",
    "    print(f\"  Premise: {ex['premise'][:80]}...\")\n",
    "    print(f\"  Gold: {ex['gold_labels']}\")\n",
    "    print(f\"  Predicted: {ex['predicted_labels']}\")\n",
    "    print(f\"  Score: {ex['disagreement_score']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f13fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error types captured in 50 samples:\n",
      "category  error_type \n",
      "housing   FN (missed)    1\n",
      "smoking   FN (missed)    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===========================Sanity Check=====================================\n",
    "# Check what error types we captured in our 50 samples\n",
    "# Analyze error patterns\n",
    "# ============================================================================\n",
    "error_analysis = []\n",
    "for idx, row in test_results_wrapped[test_results_wrapped['disagreement_score'] > 0].iterrows():\n",
    "    for cat, error_type in row['errors'].items():\n",
    "        error_analysis.append({\n",
    "            'category': cat,\n",
    "            'error_type': error_type\n",
    "        })\n",
    "\n",
    "error_df = pd.DataFrame(error_analysis)\n",
    "print(\"Error types captured in 50 samples:\")\n",
    "print(error_df.groupby(['category', 'error_type']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826fe2fd",
   "metadata": {},
   "source": [
    "***Analysis***<br>\n",
    "Concern 1: 50 Samples - Representative Enough?\n",
    "    About the 50 Sample Size:\n",
    "    Let's think statistically:\n",
    "\n",
    "    HuggingFace data: 500+ samples\n",
    "    50 samples = 10%\n",
    "    Paper used 100 from thousands â‰ˆ 5-10%\n",
    "    Proportion is similar! âœ…\n",
    "Concern 2: are categories representative Enough?\n",
    "    Rationale:\n",
    "        The top categories (smoking, alcohol, employment) are well-represented\n",
    "        The missing 3 are probably very rare in your full 500+ dataset anyway\n",
    "        Hard examples will teach: \"Don't over-predict\" (the main problem)\n",
    "        Paper didn't require ALL categories represented\n",
    "    Conclusion:\n",
    "        The 50 sample analysis is solid because:\n",
    "\n",
    "        Error pattern is clear: FP on negations (the top 2 hard examples with score=3)\n",
    "        Major categories covered: smoking, alcohol, employment\n",
    "        Matches paper proportion: 10% of your data\n",
    "        Rare categories (food, transportation, opioids) might have too few samples anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a55beec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold label distribution in 50 samples:\n",
      "Counter({'smoking': 12, 'alcohol': 6, 'employment': 4, 'housing': 1, 'marijuana': 1, 'cocaine': 1, 'drug_use': 1})\n"
     ]
    }
   ],
   "source": [
    "# Check category coverage in your 50 samples\n",
    "print(\"Gold label distribution in 50 samples:\")\n",
    "all_categories_in_50 = []\n",
    "for labels in test_results_wrapped['gold_labels']:\n",
    "    all_categories_in_50.extend(parse_llm_labels(labels))\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(all_categories_in_50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6600390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Hard Example 1:\n",
      "======================================================================\n",
      "Premise: He denies any smoking or drug usage.\n",
      "Gold: ['smoking']\n",
      "Predicted: []\n",
      "Score: 1\n",
      "\n",
      "Generating explanation...\n",
      "\n",
      "Explanation:\n",
      "The model was wrong because it failed to recognize that the phrase \"denies any smoking\" still indicates the presence of the concept \"smoking\" in the text, despite the negation. The key phrase \"denies any smoking\" explicitly references smoking behavior, which should be annotated regardless of negation. The model should follow the rule to annotate all mentioned clinical concepts, even if they are negated, to accurately capture the full scope of relevant information.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Hard Example 2:\n",
      "======================================================================\n",
      "Premise: He currently lives with his mother in house for several both<br> in New York and here in Colorado\n",
      "Gold: ['housing']\n",
      "Predicted: []\n",
      "Score: 1\n",
      "\n",
      "Generating explanation...\n",
      "\n",
      "Explanation:\n",
      "The model missed the annotation because it failed to recognize that \"lives with his mother in house\" indicates a housing-related context. The key phrase \"lives with his mother in house\" signals information about living arrangements, which should be labeled as 'housing.' To avoid this error, the model should associate phrases describing living situations or residence with the 'housing' category, even if the wording is informal or incomplete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# Step 2b: Generate explanations for both hard examples\n",
    "# ================================================================================= \n",
    "for i, hard_ex in enumerate(hard_examples, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Hard Example {i}:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Premise: {hard_ex['premise']}\")\n",
    "    print(f\"Gold: {hard_ex['gold_labels']}\")\n",
    "    print(f\"Predicted: {hard_ex['predicted_labels']}\")\n",
    "    print(f\"Score: {hard_ex['disagreement_score']}\")\n",
    "    print(f\"\\nGenerating explanation...\")\n",
    "    \n",
    "    explanation = generate_explanation_for_hard_example(\n",
    "        premise=hard_ex['premise'],\n",
    "        gold_labels=parse_llm_labels(hard_ex['gold_labels']),\n",
    "        predicted_labels=hard_ex['predicted_labels'],\n",
    "        categories=categories,\n",
    "        deployment_name=\"gpt-35-turbo\"\n",
    "    )\n",
    "    \n",
    "    hard_ex['explanation'] = explanation\n",
    "    print(f\"\\nExplanation:\")\n",
    "    print(explanation)\n",
    "    print()\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting\n",
    "# The Psychology of LLMs:\n",
    "#     0-Shot (Blind Guessing):\n",
    "# With Correct Answer (Reflective Mode):\n",
    "# This is EXACTLY Why 2-Shot Learning Works!\n",
    "# 0-shot: Model uses shallow pattern matching\n",
    "# 2-shot with explanations: Model learns to think deeper\n",
    "# Result: Model starts paying attention to negations!\n",
    "\n",
    "# ðŸ“š Why This Happens (Research Findings):\n",
    "# 1. Recency Bias:\n",
    "\n",
    "# LLMs pay more attention to things closer to the answer\n",
    "# Your rules are 10+ lines before the premise\n",
    "# By the time it reads \"Does not smoke\", Rule 6 is \"forgotten\"\n",
    "\n",
    "# 2. Instruction Hierarchy:\n",
    "\n",
    "# Rule 0 says: \"Identify all applicable categories\"\n",
    "# Model thinks: \"Find categories!\" (action-oriented)\n",
    "# Negation is exception/constraint (less salient)\n",
    "\n",
    "# 3. Token-Level Pattern Matching:\n",
    "\n",
    "# \"smoke\", \"drink\", \"drug\" have strong embeddings â†’ categories\n",
    "# \"Does not\" is a modifier (weaker signal)\n",
    "# GPT-3.5 (non-reasoning model) doesn't do multi-step logic:\n",
    "\n",
    "# Step 1: Check for negation\n",
    "# Step 2: If negation, suppress categories\n",
    "# It just pattern matches! ðŸ¤·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5faf1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert annotator for clinical text.\n",
      "\n",
      "Task:\n",
      "Given a patient note (premise), identify which of the following categories apply:\n",
      "marijuana, food, cocaine, transportation, housing, drug_use, opioids, smoking, employment, alcohol\n",
      "\n",
      "Rules:\n",
      "0) Identify all applicable categories from the list above that apply to the patient note.\n",
      "Return ONLY a JSON array of applicable categories, with NO explanation.\n",
      "1) UMBRELLA DRUG RULE: If opioids, marijuana, or cocaine are present, ALSO include \"drug_use\".\n",
      "2) EMPLOYMENT: Label ONLY if unemployed, job loss, or work problems. Do NOT label if working, retired, student, or homemaker.\n",
      "3) HOUSING: Label only if unstable (homeless, shelter, unsafe). Do NOT label for neutral mentions.\n",
      "4) SUBSTANCE USE: Include if use is stated (current or past). Negations cancel it.\n",
      "5) FOOD & TRANSPORTATION: Label only if lack/barrier exists.\n",
      "6) NEGATIONS: If \"never\", \"denies\", \"none\", \"no history\" or any negative words â†’ do NOT include that category.\n",
      "7) AMBIGUITY: If speculative (\"may use\", \"possibly\") â†’ return [].\n",
      "\n",
      "Here are two challenging examples to guide you:\n",
      "\n",
      "Example 1:\n",
      "Premise: \"No history of ETOH, Tobacco or illicit drug use\"\n",
      "Answer: []\n",
      "Explanation: The model incorrectly predicted the presence of alcohol, smoking, and drug use in the text because it failed to recognize the negation \"No history of.\" The key phrase that determines the correct answer is \"No history of,\" which indicates the absence of alcohol, tobacco, and illicit drug use. The model should be trained to identify negation cues like \"no\" or \"not\" to accurately interpret the absence of certain behaviors or conditions in clinical text annotations.\n",
      "\n",
      "Example 2:\n",
      "Premise: \"Uses no tobacco, alcohol, or illicit drugs\"\n",
      "Answer: []\n",
      "Explanation: The model incorrectly classified this text because it failed to recognize the negation \"no\" before tobacco, alcohol, and illicit drugs. The key phrase that determines the correct answer is \"no tobacco, alcohol, or illicit drugs,\" indicating that the individual does not use any of these substances. The model should follow the rule of paying attention to negations like \"no\" to accurately classify text annotations.\n",
      "\n",
      "Now classify this text:\n",
      "Premise: \"Patient denies smoking or alcohol use.\"\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Create a test prompt\n",
    "test_premise = \"Patient denies smoking or alcohol use.\"\n",
    "test_prompt = create_2shot_hard_prompt(hard_examples, categories, test_premise)\n",
    "\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4af96",
   "metadata": {},
   "source": [
    "how to split data: Use ~200 samples for testing<br>\n",
    "    - Power Analysis:\n",
    "        200 samples gives 80% statistical power to detect 10% difference in F1 (Î±=0.05)\n",
    "        Sufficient for Cohen's kappa reliability (Â±0.05 confidence interval)\n",
    "    - Cost-Benefit:\n",
    "        200 samples Ã— 3 configs = 600 API calls\n",
    "        Cost: ~$1-2\n",
    "        Time: ~10-15 minutes\n",
    "        Good balance!\n",
    "    - Category Coverage:\n",
    "        Major categories (smoking, alcohol, employment): 40-80 samples each âœ…\n",
    "        Rare categories: Still limited but best we can do with your data size\n",
    "Reserve Data:\n",
    "    Keeps ~250 samples unused\n",
    "    Can use later for:\n",
    "XGBoost training (like the paper)\n",
    "Validation\n",
    "Future experiments\n",
    "\n",
    "ðŸ“Š Confidence Intervals with 200 Samples:\n",
    "Metric          95%CI Width Interpretation\n",
    "F1 Score        Â±0.05       Good precisionCohen's \n",
    "Kappa           Â±0.05       Reliable agreement\n",
    "Per-category F1 Â±0.10-0.15  Acceptable for major categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 200\n",
      "Reserved for future: 320\n",
      "\n",
      "Category distribution in test set:\n",
      "Counter({'alcohol': 33, 'smoking': 33, 'drug_use': 7, 'marijuana': 3, 'employment': 3, 'opioids': 2, 'cocaine': 2, 'housing': 1, 'transportation': 1})\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# Step 2c: Split your remaining data\n",
    "# ================================================================================= \n",
    "results_50=test_results\n",
    "remaining_df = df_gold.drop(results_50.index)  # Remove the 50 used for hard examples\n",
    "\n",
    "# Randomly sample 200 for testing\n",
    "test_df = remaining_df.sample(n=200, random_state=42)\n",
    "\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Reserved for future: {len(remaining_df) - len(test_df)}\")\n",
    "\n",
    "# Check category distribution in test set\n",
    "test_categories = []\n",
    "for labels in test_df['gold_labels']:\n",
    "    test_categories.extend(parse_llm_labels(labels))\n",
    "\n",
    "from collections import Counter\n",
    "print(\"\\nCategory distribution in test set:\")\n",
    "print(Counter(test_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy Positive Example (Model got it RIGHT):\n",
      "  Premise: The patient smokes about a pack a day for more than 25 years...\n",
      "  Gold: ['smoking']\n",
      "  Predicted: ['smoking']\n",
      "  Score: 0\n",
      "\n",
      "Easy Negative Example (Model got it RIGHT):\n",
      "  Premise: She has never used illicit drugs...\n",
      "  Gold: []\n",
      "  Predicted: []\n",
      "  Score: 0\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# Step 3: First, Select Easy Examples\n",
    "# Select easy examples from your 50-sample results\n",
    "# =================================================================================\n",
    "easy_positive, easy_negative = select_easy_examples_automatically(results_50)\n",
    "\n",
    "# Check what we got\n",
    "print(\"Easy Positive Example (Model got it RIGHT):\")\n",
    "print(f\"  Premise: {easy_positive['premise'][:80]}...\")\n",
    "print(f\"  Gold: {easy_positive['gold_labels']}\")\n",
    "print(f\"  Predicted: {easy_positive['predicted_labels']}\")\n",
    "print(f\"  Score: {easy_positive['disagreement_score']}\")\n",
    "print()\n",
    "\n",
    "print(\"Easy Negative Example (Model got it RIGHT):\")\n",
    "print(f\"  Premise: {easy_negative['premise'][:80]}...\")\n",
    "print(f\"  Gold: {easy_negative['gold_labels']}\")\n",
    "print(f\"  Predicted: {easy_negative['predicted_labels']}\")\n",
    "print(f\"  Score: {easy_negative['disagreement_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c5e3b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating explanations for easy examples...\n",
      "\n",
      "Easy Positive Explanation:\n",
      "The key phrase in this text that determines the classification as 'smoking' is \"smokes about a pack a day for more than 25 years.\" This answer is correct because it directly mentions the patient's smoking habit, which is a significant risk factor for various health conditions. In similar cases, the model should look for specific mentions of smoking behavior, duration, and intensity to accurately classify the text as related to smoking.\n",
      "\n",
      "Easy Negative Explanation:\n",
      "The key phrase in this text is \"never used illicit drugs.\" This indicates that the individual has not engaged in the use of illegal substances. The absence of illicit drug use is a significant factor in determining the classification of this text as negative for drug use. Models should look for explicit statements regarding drug use to make accurate classifications in similar cases.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating explanations for easy examples...\")\n",
    "print()\n",
    "\n",
    "# Easy positive explanation\n",
    "easy_pos_explanation = generate_explanation_for_easy_example(\n",
    "    premise=easy_positive['premise'],\n",
    "    gold_labels=parse_llm_labels(easy_positive['gold_labels']),\n",
    "    predicted_labels=easy_positive['predicted_labels'],\n",
    "    categories=categories,\n",
    "    deployment_name=\"gpt-35-turbo\"\n",
    ")\n",
    "easy_positive['explanation'] = easy_pos_explanation\n",
    "\n",
    "print(\"Easy Positive Explanation:\")\n",
    "print(easy_pos_explanation)\n",
    "print()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Easy negative explanation\n",
    "easy_neg_explanation = generate_explanation_for_easy_example(\n",
    "    premise=easy_negative['premise'],\n",
    "    gold_labels=parse_llm_labels(easy_negative['gold_labels']),\n",
    "    predicted_labels=easy_negative['predicted_labels'],\n",
    "    categories=categories,\n",
    "    deployment_name=\"gpt-35-turbo\"\n",
    ")\n",
    "easy_negative['explanation'] = easy_neg_explanation\n",
    "\n",
    "print(\"Easy Negative Explanation:\")\n",
    "print(easy_neg_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a8d9e",
   "metadata": {},
   "source": [
    "***Save progress***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32e46652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint!\n",
      "âœ“ Hard examples: 2\n",
      "âœ“ Easy positive: The patient smokes about a pack a day for more tha...\n",
      "âœ“ Easy negative: She has never used illicit drugs...\n",
      "âœ“ Test set size: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your saved progress\n",
    "save_data = {\n",
    "    'hard_examples': hard_examples,\n",
    "    'easy_positive': easy_positive,\n",
    "    'easy_negative': easy_negative,\n",
    "    'test_df': test_df,\n",
    "    'results_50': results_50,\n",
    "    'categories': categories\n",
    "}\n",
    "with open('procgress_checkpoint_step61.pkl', 'rb') as f:\n",
    "    save_data = pickle.load(f)\n",
    "\n",
    "# Extract the variables\n",
    "hard_examples = save_data['hard_examples']\n",
    "easy_positive = save_data['easy_positive']\n",
    "easy_negative = save_data['easy_negative']\n",
    "test_df = save_data['test_df']\n",
    "results_50 = save_data['results_50']\n",
    "categories = save_data['categories']\n",
    "# Verify what we loaded\n",
    "print(\"Loaded checkpoint!\")\n",
    "print(f\"âœ“ Hard examples: {len(hard_examples)}\")\n",
    "print(f\"âœ“ Easy positive: {easy_positive['premise'][:50]}...\")\n",
    "print(f\"âœ“ Easy negative: {easy_negative['premise'][:50]}...\")\n",
    "print(f\"âœ“ Test set size: {len(test_df)}\")\n",
    "print()\n",
    "\n",
    "# Package easy examples as tuple for the function\n",
    "easy_examples = (easy_positive, easy_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b79cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# Step 4: Create the Full Evaluation Function \n",
    "# Now let's put it all together and evaluate all 5 configurations on your test set:\n",
    "# =================================================================================\n",
    "def evaluate_all_configurations(test_df, easy_examples, hard_examples, \n",
    "                                categories, deployment_name=\"gpt-35-turbo\"):\n",
    "    \"\"\"\n",
    "    Evaluate ALL 5 configurations:\n",
    "    1. 0-Shot\n",
    "    2. 2-Shot Easy (no explanation)\n",
    "    3. 2-Shot Easy (with explanation)\n",
    "    4. 2-Shot Hard (no explanation)\n",
    "    5. 2-Shot Hard (with explanation)\n",
    "    \"\"\"\n",
    "    from llm_utils import run_llm_annotation, parse_llm_labels\n",
    "    \n",
    "    results = {}\n",
    "    easy_positive, easy_negative = easy_examples\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"STARTING 5-CONFIGURATION EVALUATION\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Config 1: 0-Shot\n",
    "    print(\"\\n[1/5] Running 0-Shot...\")\n",
    "    preds_0shot = run_llm_annotation(\n",
    "        df=test_df,\n",
    "        categories=categories,\n",
    "        examples=\"\",\n",
    "        deployment_name=deployment_name,\n",
    "        sleep_time=1\n",
    "    )\n",
    "    results['0-Shot'] = preds_0shot\n",
    "    print(\"âœ“ 0-Shot complete\")\n",
    "    \n",
    "    # Config 2: 2-Shot Easy (no exp)\n",
    "    print(\"\\n[2/5] Running 2-Shot Easy (no explanations)...\")\n",
    "    examples_easy_no_exp = f\"\"\"\n",
    "Example 1 (Positive):\n",
    "Premise: \"{easy_positive['premise']}\"\n",
    "Answer: {parse_llm_labels(easy_positive['gold_labels'])}\n",
    "\n",
    "Example 2 (Negative):\n",
    "Premise: \"{easy_negative['premise']}\"\n",
    "Answer: {parse_llm_labels(easy_negative['gold_labels'])}\n",
    "\"\"\"\n",
    "    preds_2easy_no_exp = run_llm_annotation(\n",
    "        df=test_df,\n",
    "        categories=categories,\n",
    "        examples=examples_easy_no_exp,\n",
    "        deployment_name=deployment_name,\n",
    "        sleep_time=1\n",
    "    )\n",
    "    results['2-Shot Easy'] = preds_2easy_no_exp\n",
    "    print(\"âœ“ 2-Shot Easy complete\")\n",
    "    \n",
    "    # Config 3: 2-Shot Easy (with exp)\n",
    "    print(\"\\n[3/5] Running 2-Shot Easy (with explanations)...\")\n",
    "    examples_easy_with_exp = f\"\"\"\n",
    "Example 1 (Positive):\n",
    "Premise: \"{easy_positive['premise']}\"\n",
    "Answer: {parse_llm_labels(easy_positive['gold_labels'])}\n",
    "Explanation: {easy_positive['explanation']}\n",
    "\n",
    "Example 2 (Negative):\n",
    "Premise: \"{easy_negative['premise']}\"\n",
    "Answer: {parse_llm_labels(easy_negative['gold_labels'])}\n",
    "Explanation: {easy_negative['explanation']}\n",
    "\"\"\"\n",
    "    preds_2easy_with_exp = run_llm_annotation(\n",
    "        df=test_df,\n",
    "        categories=categories,\n",
    "        examples=examples_easy_with_exp,\n",
    "        deployment_name=deployment_name,\n",
    "        sleep_time=1\n",
    "    )\n",
    "    results['2-Shot Easy + Exp'] = preds_2easy_with_exp\n",
    "    print(\"âœ“ 2-Shot Easy + Exp complete\")\n",
    "    \n",
    "    # Config 4: 2-Shot Hard (no exp)\n",
    "    print(\"\\n[4/5] Running 2-Shot Hard (no explanations)...\")\n",
    "    examples_hard_no_exp = f\"\"\"\n",
    "Example 1 (Negative):\n",
    "Premise: \"{hard_examples[0]['premise']}\"\n",
    "Answer: {parse_llm_labels(hard_examples[0]['gold_labels'])}\n",
    "\n",
    "Example 2 (Positive):\n",
    "Premise: \"{hard_examples[1]['premise']}\"\n",
    "Answer: {parse_llm_labels(hard_examples[1]['gold_labels'])}\n",
    "\"\"\"\n",
    "    preds_2hard_no_exp = run_llm_annotation(\n",
    "        df=test_df,\n",
    "        categories=categories,\n",
    "        examples=examples_hard_no_exp,\n",
    "        deployment_name=deployment_name,\n",
    "        sleep_time=1\n",
    "    )\n",
    "    results['2-Shot Hard'] = preds_2hard_no_exp\n",
    "    print(\"âœ“ 2-Shot Hard complete\")\n",
    "    \n",
    "    # Config 5: 2-Shot Hard (with exp)\n",
    "    print(\"\\n[5/5] Running 2-Shot Hard (with explanations)...\")\n",
    "    examples_hard_with_exp = f\"\"\"\n",
    "Example 1 (Negative):\n",
    "Premise: \"{hard_examples[0]['premise']}\"\n",
    "Answer: {parse_llm_labels(hard_examples[0]['gold_labels'])}\n",
    "Explanation: {hard_examples[0]['explanation']}\n",
    "\n",
    "Example 2 (Positive):\n",
    "Premise: \"{hard_examples[1]['premise']}\"\n",
    "Answer: {parse_llm_labels(hard_examples[1]['gold_labels'])}\n",
    "Explanation: {hard_examples[1]['explanation']}\n",
    "\"\"\"\n",
    "    preds_2hard_with_exp = run_llm_annotation(\n",
    "        df=test_df,\n",
    "        categories=categories,\n",
    "        examples=examples_hard_with_exp,\n",
    "        deployment_name=deployment_name,\n",
    "        sleep_time=1\n",
    "    )\n",
    "    results['2-Shot Hard + Exp'] = preds_2hard_with_exp\n",
    "    print(\"âœ“ 2-Shot Hard + Exp complete\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ALL 5 CONFIGURATIONS COMPLETE! ðŸŽ‰\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a69a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with 10 samples first...\n",
      "Small test set size: 10\n",
      "\n",
      "======================================================================\n",
      "STARTING 5-CONFIGURATION EVALUATION\n",
      "Test samples: 10\n",
      "======================================================================\n",
      "\n",
      "[1/5] Running 0-Shot...\n",
      "âœ“ 0-Shot complete\n",
      "\n",
      "[2/5] Running 2-Shot Easy (no explanations)...\n",
      "âœ“ 2-Shot Easy complete\n",
      "\n",
      "[3/5] Running 2-Shot Easy (with explanations)...\n",
      "âœ“ 2-Shot Easy + Exp complete\n",
      "\n",
      "[4/5] Running 2-Shot Hard (no explanations)...\n",
      "âœ“ 2-Shot Hard complete\n",
      "\n",
      "[5/5] Running 2-Shot Hard (with explanations)...\n",
      "âœ“ 2-Shot Hard + Exp complete\n",
      "\n",
      "======================================================================\n",
      "ALL 5 CONFIGURATIONS COMPLETE! ðŸŽ‰\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "QUICK RESULTS CHECK\n",
      "======================================================================\n",
      "0-Shot: 10 predictions\n",
      "  Sample: []...\n",
      "\n",
      "2-Shot Easy: 10 predictions\n",
      "  Sample: []...\n",
      "\n",
      "2-Shot Easy + Exp: 10 predictions\n",
      "  Sample: []...\n",
      "\n",
      "2-Shot Hard: 10 predictions\n",
      "  Sample: []...\n",
      "\n",
      "2-Shot Hard + Exp: 10 predictions\n",
      "  Sample: []...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================Sanity Check=====================================v\n",
    "# Create a small test set (10 samples)\n",
    "test_df_small = test_df.sample(n=10, random_state=42)\n",
    "\n",
    "print(\"Testing with 10 samples first...\")\n",
    "print(f\"Small test set size: {len(test_df_small)}\")\n",
    "print()\n",
    "\n",
    "# Run the evaluation on small test set\n",
    "test_results = evaluate_all_configurations(\n",
    "    test_df=test_df_small,\n",
    "    easy_examples=easy_examples,\n",
    "    hard_examples=hard_examples,\n",
    "    categories=categories,\n",
    "    deployment_name=\"gpt-35-turbo\"\n",
    ")\n",
    "\n",
    "# Quick check of results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUICK RESULTS CHECK\")\n",
    "print(\"=\"*70)\n",
    "for config_name, predictions in test_results.items():\n",
    "    print(f\"{config_name}: {len(predictions)} predictions\")\n",
    "    print(f\"  Sample: {predictions[0][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41baa23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING FULL EVALUATION ON 200 SAMPLES\n",
      "Estimated time: 15-20 minutes\n",
      "Estimated cost: ~$2-3\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STARTING 5-CONFIGURATION EVALUATION\n",
      "Test samples: 200\n",
      "======================================================================\n",
      "\n",
      "[1/5] Running 0-Shot...\n",
      "âœ“ 0-Shot complete\n",
      "\n",
      "[2/5] Running 2-Shot Easy (no explanations)...\n",
      "âœ“ 2-Shot Easy complete\n",
      "\n",
      "[3/5] Running 2-Shot Easy (with explanations)...\n",
      "âœ“ 2-Shot Easy + Exp complete\n",
      "\n",
      "[4/5] Running 2-Shot Hard (no explanations)...\n",
      "âœ“ 2-Shot Hard complete\n",
      "\n",
      "[5/5] Running 2-Shot Hard (with explanations)...\n",
      "âœ“ 2-Shot Hard + Exp complete\n",
      "\n",
      "======================================================================\n",
      "ALL 5 CONFIGURATIONS COMPLETE! ðŸŽ‰\n",
      "======================================================================\n",
      "\n",
      "âœ… Results saved to 'full_evaluation_results.pkl'\n"
     ]
    }
   ],
   "source": [
    "# run 200 test samples\n",
    "print(\"=\"*70)\n",
    "print(\"RUNNING FULL EVALUATION ON 200 SAMPLES\")\n",
    "print(\"Estimated time: 15-20 minutes\")\n",
    "print(\"Estimated cost: ~$2-3\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "# Run on full 200 samples\n",
    "full_results = evaluate_all_configurations(\n",
    "    test_df=test_df,  # Use the full 200-sample test set\n",
    "    easy_examples=easy_examples,\n",
    "    hard_examples=hard_examples,\n",
    "    categories=categories,\n",
    "    deployment_name=\"gpt-35-turbo\"\n",
    ")\n",
    "import pickle\n",
    "with open('full_evaluation_results.pkl', 'wb') as f:\n",
    "    pickle.dump(full_results, f)\n",
    "\n",
    "print(\"\\nâœ… Results saved to 'full_evaluation_results.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4674cd",
   "metadata": {},
   "source": [
    "***Save Progress***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bd0f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded full evaluation results!\n",
      "Configurations evaluated: ['0-Shot', '2-Shot Easy', '2-Shot Easy + Exp', '2-Shot Hard', '2-Shot Hard + Exp']\n",
      "Samples per configuration: 200\n",
      "0-Shot:\n",
      "  Total predictions: 200\n",
      "  Sample prediction: [\"employment\"]\n",
      "2-Shot Easy:\n",
      "  Total predictions: 200\n",
      "  Sample prediction: []\n",
      "2-Shot Easy + Exp:\n",
      "  Total predictions: 200\n",
      "  Sample prediction: [\"employment\"]\n",
      "2-Shot Hard:\n",
      "  Total predictions: 200\n",
      "  Sample prediction: []\n",
      "2-Shot Hard + Exp:\n",
      "  Total predictions: 200\n",
      "  Sample prediction: []\n"
     ]
    }
   ],
   "source": [
    "# Load full evaluation results\n",
    "with open('full_evaluation_results.pkl', 'rb') as f:\n",
    "    full_results = pickle.load(f)   \n",
    "print(\"Loaded full evaluation results!\")\n",
    "print(f\"Configurations evaluated: {list(full_results.keys())}\")\n",
    "print(f\"Samples per configuration: {len(full_results['0-Shot'])}\")\n",
    "\n",
    "# sanity check\n",
    "for config_name, predictions in full_results.items():\n",
    "    print(f\"{config_name}:\")\n",
    "    print(f\"  Total predictions: {len(predictions)}\")\n",
    "    print(f\"  Sample prediction: {predictions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2170d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Parsed 0-Shot\n",
      "âœ“ Parsed 2-Shot Easy\n",
      "âœ“ Parsed 2-Shot Easy + Exp\n",
      "âœ“ Parsed 2-Shot Hard\n",
      "âœ“ Parsed 2-Shot Hard + Exp\n",
      "\n",
      "âœ“ Parsed 200 gold labels\n",
      "Sample comparison\n",
      "\n",
      "Example 1:\n",
      "   Gold: []\n",
      "   0-shot: ['employment']\n",
      "   2-shot Hard+Exp: []\n",
      "\n",
      "Example 2:\n",
      "   Gold: ['alcohol']\n",
      "   0-shot: ['alcohol']\n",
      "   2-shot Hard+Exp: ['alcohol']\n",
      "\n",
      "Example 3:\n",
      "   Gold: ['smoking']\n",
      "   0-shot: ['smoking']\n",
      "   2-shot Hard+Exp: ['smoking']\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================\n",
    "# Step 5: parse predictions and prepare for metrics\n",
    "# =================================================================================\n",
    "# prepare variables for gpt4.0 prediction\n",
    "parsed_results={}\n",
    "for config_name, raw_predictions in full_results.items():\n",
    "    parsed_results[config_name]=[\n",
    "        parse_llm_labels(pred) for pred in raw_predictions\n",
    "    ]\n",
    "    print(f\"âœ“ Parsed {config_name}\")\n",
    "print()\n",
    "\n",
    "# Parse gold labels from test_df\n",
    "gold_labels_parsed=[\n",
    "    parse_llm_labels(label) for label in test_df['gold_labels']\n",
    "]\n",
    "print(f\"âœ“ Parsed {len(gold_labels_parsed)} gold labels\")\n",
    "# Quick verification - show a few examples\n",
    "print(\"Sample comparison\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"   Gold: {gold_labels_parsed[i]}\")\n",
    "    print(f\"   0-shot: {parsed_results['0-Shot'][i]}\")\n",
    "    print(f\"   2-shot Hard+Exp: {parsed_results['2-Shot Hard + Exp'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CALCULATING METRICS FOR ALL CONFIGURATIONS\n",
      "======================================================================\n",
      "\n",
      "Calculating metrics for 0-Shot...\n",
      "  F1 (micro): 0.576\n",
      "  F1 (macro): 0.578\n",
      "  Cohen's Kappa: 0.549\n",
      "  Exact Match: 0.535\n",
      "\n",
      "Calculating metrics for 2-Shot Easy...\n",
      "  F1 (micro): 0.845\n",
      "  F1 (macro): 0.685\n",
      "  Cohen's Kappa: 0.837\n",
      "  Exact Match: 0.865\n",
      "\n",
      "Calculating metrics for 2-Shot Easy + Exp...\n",
      "  F1 (micro): 0.827\n",
      "  F1 (macro): 0.693\n",
      "  Cohen's Kappa: 0.818\n",
      "  Exact Match: 0.845\n",
      "\n",
      "Calculating metrics for 2-Shot Hard...\n",
      "  F1 (micro): 0.802\n",
      "  F1 (macro): 0.680\n",
      "  Cohen's Kappa: 0.792\n",
      "  Exact Match: 0.820\n",
      "\n",
      "Calculating metrics for 2-Shot Hard + Exp...\n",
      "  F1 (micro): 0.786\n",
      "  F1 (macro): 0.681\n",
      "  Cohen's Kappa: 0.775\n",
      "  Exact Match: 0.800\n",
      "\n",
      "======================================================================\n",
      "METRICS CALCULATION COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for all 5 configurations\n",
    "print(\"=\"*70)\n",
    "print(\"CALCULATING METRICS FOR ALL CONFIGURATIONS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "all_metrics = {}\n",
    "\n",
    "for config_name in ['0-Shot', '2-Shot Easy', '2-Shot Easy + Exp', \n",
    "                     '2-Shot Hard', '2-Shot Hard + Exp']:\n",
    "    print(f\"Calculating metrics for {config_name}...\")\n",
    "    \n",
    "    metrics = calculate_multilabel_metrics(\n",
    "        gold_labels=gold_labels_parsed,\n",
    "        predicted_labels=parsed_results[config_name],\n",
    "        categories=categories\n",
    "    )\n",
    "    \n",
    "    all_metrics[config_name] = metrics\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  F1 (micro): {metrics['f1_micro']:.3f}\")\n",
    "    print(f\"  F1 (macro): {metrics['f1_macro']:.3f}\")\n",
    "    print(f\"  Cohen's Kappa: {metrics['cohen_kappa']:.3f}\")\n",
    "    print(f\"  Exact Match: {metrics['exact_match_accuracy']:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"METRICS CALCULATION COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cbe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all_metrics\n",
    "with open('all_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(all_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb488390",
   "metadata": {},
   "source": [
    "1. SURPRISING: Easy Examples > Hard Examples!\n",
    "This is DIFFERENT from the paper!\n",
    "The paper found: Hard + Exp dominates; My results: Easy (no exp) dominates\n",
    "Why?\n",
    "\n",
    "My hard examples were BOTH about negations (score=3)\n",
    "Maybe too specific? Only taught negation handling\n",
    "Easy examples were more diverse (positive smoking case + negative empty case)\n",
    "More generalizable teaching!\n",
    "2. Explanations HURT Performance!\n",
    "**This matches the paper's \"over-regularization\" finding!**\n",
    "- Explanations make the model more conservative\n",
    "- Slightly reduces performance\n",
    "**3. ALL 2-Shot Configs Beat 0-Shot Significantly!**\n",
    "**Improvement:** +20-27% F1 score! ðŸš€\n",
    "\n",
    "**4. Cohen's Kappa Interpretation:**\n",
    "\n",
    "According to the paper and literature:\n",
    "- **0.41-0.60:** Moderate agreement\n",
    "- **0.61-0.80:** Substantial agreement  \n",
    "- **0.81-1.00:** Almost perfect agreement\n",
    "\n",
    "**My results:**\n",
    "- 0-Shot: 0.549 (Moderate) âš ï¸\n",
    "- 2-Shot Easy: **0.837 (Almost perfect!)** âœ…\n",
    "- Paper reported: 0.82-0.92 for best configs âœ…\n",
    "Why Easy Beat Hard (My Theory):\n",
    "\n",
    "**Your Hard Examples:**\n",
    "```\n",
    "1. \"Does not smoke, drink...\" â†’ [] (negation)\n",
    "2. \"Uses no tobacco, alcohol...\" â†’ [] (negation)\n",
    "```\n",
    "- Both teach: \"Negations â†’ empty\"\n",
    "- Too narrow! Only fixes one error type\n",
    "\n",
    "**Your Easy Examples:**\n",
    "```\n",
    "1. \"Smokes a pack a day...\" â†’ ['smoking'] (clear positive)\n",
    "2. \"Never used illicit drugs\" â†’ [] (clear negative)\n",
    "Teaches BOTH: When to predict AND when not to predict\n",
    "More balanced training!\n",
    "\n",
    "ðŸŽ¯ Conclusion:\n",
    "Your experiment actually reveals something important:\n",
    "âœ… 2-shot learning works (huge improvement over 0-shot)\n",
    "âœ… Cohen's Kappa 0.837 matches paper quality (0.82-0.92)\n",
    "âš ï¸ Hard examples need diversity - yours were too focused on negations\n",
    "âš ï¸ Explanations help consistency but may over-regularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11c67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 4-Shot on 10 samples...\n",
      "Got 10 predictions!\n"
     ]
    }
   ],
   "source": [
    "#=================================================================================\n",
    "# Create 4-shot examples to improve performance\n",
    "#=================================================================================\n",
    "def create_4shot_prompt(easy_examples, hard_examples):\n",
    "    easy_pos, easy_neg = easy_examples\n",
    "    \n",
    "    examples_4shot = f\"\"\"\n",
    "Example 1 (Easy Positive):\n",
    "Premise: \"{easy_pos['premise']}\"\n",
    "Answer: {parse_llm_labels(easy_pos['gold_labels'])}\n",
    "\n",
    "Example 2 (Easy Negative):\n",
    "Premise: \"{easy_neg['premise']}\"\n",
    "Answer: {parse_llm_labels(easy_neg['gold_labels'])}\n",
    "\n",
    "Example 3 (Hard Negative - Tricky):\n",
    "Premise: \"{hard_examples[0]['premise']}\"\n",
    "Answer: {parse_llm_labels(hard_examples[0]['gold_labels'])}\n",
    "Explanation: {hard_examples[0]['explanation']}\n",
    "\n",
    "Example 4 (Hard Positive - Tricky):\n",
    "Premise: \"{hard_examples[1]['premise']}\"\n",
    "Answer: {parse_llm_labels(hard_examples[1]['gold_labels'])}\n",
    "Explanation: {hard_examples[1]['explanation']}\n",
    "\"\"\"\n",
    "    return examples_4shot\n",
    "\n",
    "# Test on 10 samples first\n",
    "print(\"Testing 4-Shot on 10 samples...\")\n",
    "test_10 = test_df.sample(n=10, random_state=99)\n",
    "\n",
    "examples_4shot = create_4shot_prompt(easy_examples, hard_examples)\n",
    "preds_4shot = run_llm_annotation(\n",
    "    df=test_10,\n",
    "    categories=categories,\n",
    "    examples=examples_4shot,\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    sleep_time=1\n",
    ")\n",
    "\n",
    "print(f\"Got {len(preds_4shot)} predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b049b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RUNNING 4-SHOT EVALUATION ON 200 SAMPLES\n",
      "======================================================================\n",
      "\n",
      "4-Shot Results:\n",
      "  F1 (micro): 0.815\n",
      "  F1 (macro): 0.668\n",
      "  Cohen's Kappa: 0.806\n",
      "  Exact Match: 0.830\n",
      "\n",
      "Comparison:\n",
      "  2-Shot Easy: F1=0.845\n",
      "  4-Shot:      F1=0.815\n",
      "  Improvement: -3.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RUNNING 4-SHOT EVALUATION ON 200 SAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run on full test set\n",
    "examples_4shot = create_4shot_prompt(easy_examples, hard_examples)\n",
    "preds_4shot_full = run_llm_annotation(\n",
    "    df=test_df,\n",
    "    categories=categories,\n",
    "    examples=examples_4shot,\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    sleep_time=1\n",
    ")\n",
    "\n",
    "# Parse predictions\n",
    "parsed_4shot = [parse_llm_labels(pred) for pred in preds_4shot_full]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_4shot = calculate_multilabel_metrics(\n",
    "    gold_labels=gold_labels_parsed,\n",
    "    predicted_labels=parsed_4shot,\n",
    "    categories=categories\n",
    ")\n",
    "\n",
    "print(\"\\n4-Shot Results:\")\n",
    "print(f\"  F1 (micro): {metrics_4shot['f1_micro']:.3f}\")\n",
    "print(f\"  F1 (macro): {metrics_4shot['f1_macro']:.3f}\")\n",
    "print(f\"  Cohen's Kappa: {metrics_4shot['cohen_kappa']:.3f}\")\n",
    "print(f\"  Exact Match: {metrics_4shot['exact_match_accuracy']:.3f}\")\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  2-Shot Easy: F1={all_metrics['2-Shot Easy']['f1_micro']:.3f}\")\n",
    "print(f\"  4-Shot:      F1={metrics_4shot['f1_micro']:.3f}\")\n",
    "print(f\"  Improvement: {(metrics_4shot['f1_micro'] - all_metrics['2-Shot Easy']['f1_micro'])*100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01397af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
